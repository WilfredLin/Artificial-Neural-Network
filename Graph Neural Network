import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch_geometric.data import Data, DataLoader
from torch_geometric.nn import GCNConv

class GNN(nn.Module):
    def __init__(self, num_features, num_classes):
        super(GNN, self).__init__()
        self.conv1 = GCNConv(num_features, 16)
        self.conv2 = GCNConv(16, num_classes)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

# Example usage
if __name__ == "__main__":
    # Create synthetic graph data
    num_nodes = 100
    num_features = 10
    num_classes = 3

    # Random features for each node
    x = torch.rand((num_nodes, num_features), dtype=torch.float)

    # Random edges (fully connected for simplicity)
    edge_index = torch.randint(0, num_nodes, (2, num_nodes * 2), dtype=torch.long)

    # Create graph data
    data = Data(x=x, edge_index=edge_index)

    # Create DataLoader
    loader = DataLoader([data], batch_size=1)

    # Initialize model, optimizer, and loss function
    model = GNN(num_features=num_features, num_classes=num_classes)
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()

    # Training loop (for demonstration)
    model.train()
    for epoch in range(100):
        for batch in loader:
            optimizer.zero_grad()
            out = model(batch)
            # Random target for demonstration
            target = torch.randint(0, num_classes, (num_nodes,))
            loss = criterion(out, target)
            loss.backward()
            optimizer.step()
        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')

    # Example inference
    model.eval()
    with torch.no_grad():
        out = model(data)
        print(out)
